---
title: "Calibration Plots"
date: now
author:
  - name: Wooyong Park
    email: wooyongp@stanford.edu
format:
  html:
    code-overflow: scroll
    toc: true
    html-math-method: katex
    css: styles.css
    theme: cosmo
execute:
  python:
    virtualenv: ".venv"
---


## Introduction

**Calibration plots**, aka reliability plots, display the empirical probability against the predicted probability. The dotted points in these plots represent these probabilities from the trained model, and ideally, they should lie on the 45-degree line; if so, we say that the model is well-calibrated. A well-crafted calibration plot helps us understand the performance of a model and compare it to other models.

We will cover how to create calibration plots for different types of outcomes in this tutorial. Let's begin with loading the necessary packages and setting the seed. This practice tutorial works with both R and Python.

> NB: This tutorial has scrollable code blocks. Please scroll all the way down to see full code.

### Packages

:::{.panel-tabset}

#### R
```{r packages-r}
#| warning: false
#| message: false

# load packages
library(tibble)
library(ggplot2)
library(dplyr)
library(readr)
library(grf)
library(nnet)
library(e1071)
library(plotly)
library(shiny)
library(htmlwidgets)
library(DT)
library(kableExtra)
library(patchwork)

# set seed
set.seed(42)
```

#### Python
```{python packages-python}
# load packages
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.calibration import calibration_curve
from sklearn.linear_model import LogisticRegression
import ipywidgets as widgets
from IPython.display import display
import patchworklib as pw
# set seed
np.random.seed(42)
```

:::

## Binary Outcomes

### Data

We’ll use the abridged version of the General Social Survey (GSS) [(Smith, 2016)](https://gss.norc.org/content/dam/gss/get-documentation/pdf/reports/project-reports/GSSProject%20report32.pdf) dataset. In this dataset, individuals were sent to treatment or control with equal probability, so we are in a randomized setting. 

>  Individuals were asked about their thoughts on government spending on the social safety net. The treatment is the wording of the question: about half of the individuals were asked if they thought government spends too much on “welfare” $(W_i=1)$, while the remaining half was asked about “assistance to the poor”$(W_i=0)$. The outcome is binary, with $Y_i=1$ corresponding to a positive answer. In the data set below, we also collect a few other demographic covariates.


:::{.panel-tabset}

#### R

```{r data-r}

# Read in data
data <- read.csv("https://docs.google.com/uc?id=1AQva5-vDlgBcM_Tv9yrO8yMYRfQJgqo_&export=download")
n <- nrow(data)


# Treatment: does the the gov't spend too much on "welfare" (1) or "assistance to the poor" (0)
treatment <- "w"

# Outcome: 1 for 'yes', 0 for 'no'
outcome <- "y"

# Additional covariates
covariates <- c("age", "polviews", "income", "educ", "marital", "sex")
```

#### Python
```{python data-python}

# Read in data
data = pd.read_csv("https://docs.google.com/uc?id=1AQva5-vDlgBcM_Tv9yrO8yMYRfQJgqo_&export=download")
n = len(data)

# Treatment: does the the gov't spend too much on "welfare" (1) or "assistance to the poor" (0)
treatment = "w"
outcome = "y"

# Additional covariates
covariates = ["age", "polviews", "income", "educ", "marital", "sex"]
```

:::


### Predictions

Within the control group, we will fit a random forest model and a Naive Bayes classifier to predict the outcome $Y$. To do this properly, we will need to divide the control group into training and testing sets. This step is especially important for calibration plots. A likely mistake is to fit the model on the entire control group and then use the fitted model to create the calibration plots, but such a procedure is equivalent to reporting the model performance(e.g., MSE) on the entire control group, which is not a good measure of model performance.

For reproducibility, we will use the first 10,000 observations in the control group to fit the models and the remaining observations, which can be considered the testing set, to create the calibration plots.

:::{.panel-tabset}

#### R
```{r predictions-r}

# Data preparation
data_c <- data[data[,treatment] == 0,] |> select(all_of(c(outcome, covariates)))
n_control <- nrow(data_c)
data_c_training <- data_c[1:10000,]
data_c_testing <- data_c[10001:n_control,]


# Random forest
forest <- grf::regression_forest(
  X = data_c_training[,covariates], 
  Y = data_c_training[,outcome], 
  honesty = FALSE,
  tune.parameters = "all", 
  num.trees = 1000, 
  seed = 42
  )

# Naive Bayes
nb <- naiveBayes(
  formula = as.formula(paste0(outcome, " ~ ", paste0(covariates, collapse = "+"))),
  data = data_c_training
)
```

#### Python
```{python predictions-python}

# Data preparation
data_c = data[data[treatment] == 0][[outcome, *covariates]]
n_control = len(data_c)
data_c_training = data_c.iloc[:10000]
data_c_testing = data_c.iloc[10001:n_control]

# Random forest
forest = RandomForestClassifier(random_state=42, n_estimators=1000)
forest.fit(data_c_training[covariates], data_c_training[outcome])

# Naive Bayes
nb = GaussianNB()
nb.fit(data_c_training[covariates], data_c_training[outcome])

```

:::

### Calibration Plots

Now let's create interactive calibration plots where you can choose the number of bins. First, we'll generate predictions on the test data and create calibration plots for both the Random Forest and Naive Bayes models.

:::{.panel-tabset}

#### R

```{r predictions-test-r}
# Generate predictions on test data
forest_pred <- predict(forest, data_c_testing[,covariates])$predictions
nb_pred <- predict(nb, data_c_testing, type = "raw")[,2]

# Create data frame for plotting
plot_data <- data.frame(
  actual = data_c_testing[,outcome],
  forest_pred = forest_pred,
  nb_pred = nb_pred
)

# Display Predictions vs Actual
print("Predictions vs Actual Data:")
kableExtra::kable(plot_data) |> kable_styling(bootstrap_options = c("striped", "hover")) |> scroll_box(width = "100%", height = "500px")
```

```{r calibration-plot-r}
#| fig-width: 12
#| fig-height: 8

# Simple calibration plot with 10 bins
n_bins <- 10

# Bin the predictions
rf_bins <- cut(plot_data$forest_pred, breaks = n_bins, labels = FALSE, include.lowest = TRUE)
nb_bins <- cut(plot_data$nb_pred, breaks = n_bins, labels = FALSE, include.lowest = TRUE)

# Calculate the statistics for the Random Forest model
rf_bin_stats <- tibble(
    bin_center = tapply(plot_data$forest_pred, rf_bins, mean),
    empirical_prob = tapply(plot_data$actual, rf_bins, mean),
    count = as.numeric(table(rf_bins)),
    empirical_se = tapply(plot_data$actual, rf_bins, sd)/count
  )

# Calculate the statistics for the Naive Bayes model
nb_bin_stats <- tibble(
    bin_center = tapply(plot_data$nb_pred, nb_bins, mean),
    empirical_prob = tapply(plot_data$actual, nb_bins, mean),
    count = as.numeric(table(nb_bins)),
    empirical_se = tapply(plot_data$actual, nb_bins, sd)/count
  )

# Combine the statistics for the two models
bin_stats <-  mutate(rf_bin_stats, model = "Random Forest") |> 
  bind_rows(mutate(nb_bin_stats, model = "Naive Bayes"))

# Create the calibration plot
ggplot(bin_stats) +
  geom_abline(aes(slope = 1, intercept = 0, color = "Perfect Calibration"), linetype = "dashed") +
  geom_ribbon(aes(x = bin_center, y = empirical_prob, fill = model,
                    ymin = empirical_prob - 1.96 * empirical_se, 
                    ymax = empirical_prob + 1.96 * empirical_se), alpha = 0.5) +
  geom_point(aes(x=bin_center, y=empirical_prob, color = model)) +
  scale_color_manual(values = c("Perfect Calibration" = "red", "Random Forest" = "blue", "Naive Bayes" = "green")) +
  scale_fill_manual(values = c("Perfect Calibration" = "red", "Random Forest" = "blue", "Naive Bayes" = "green")) +
  labs(title = paste0("Calibration Plot with ", n_bins, " bins"), x=NULL, y=NULL, color = NULL) + guides(fill = "none") +
  theme_minimal() + theme(legend.position = "bottom")
```

#### Python

```{python predictions-test-python}
# Generate predictions on test data
forest_pred = forest.predict_proba(data_c_testing[covariates])[:, 1]
nb_pred = nb.predict_proba(data_c_testing[covariates])[:, 1]

# Create data frame for plotting
plot_data = pd.DataFrame({
    'actual': data_c_testing[outcome],
    'forest_pred': forest_pred,
    'nb_pred': nb_pred
})

# Display Predictions vs Actual
print("Predictions vs Actual Data:")
output = widgets.Output()
with output:
    display(plot_data)

```

```{python calibration-plot-python}
#| fig-width: 12
#| fig-height: 8

# Simple calibration plot with 10 bins
n_bins = 10

# Bin the predictions
rf_bins = pd.cut(plot_data['forest_pred'], bins=n_bins, labels=False, include_lowest=True)
nb_bins = pd.cut(plot_data['nb_pred'], bins=n_bins, labels=False, include_lowest=True)

# Calculate the statistics for the Random Forest model
rf_bin_stats = pd.DataFrame({
    'bin_center': plot_data['forest_pred'].groupby(rf_bins).mean(),
    'empirical_prob': plot_data['actual'].groupby(rf_bins).mean(),
    'count': rf_bins.value_counts().sort_index(),
    'empirical_se': plot_data['actual'].groupby(rf_bins).std() / rf_bins.value_counts().sort_index()
})

# Calculate the statistics for the Naive Bayes model
nb_bin_stats = pd.DataFrame({
    'bin_center': plot_data['nb_pred'].groupby(nb_bins).mean(),
    'empirical_prob': plot_data['actual'].groupby(nb_bins).mean(),
    'count': nb_bins.value_counts().sort_index(),
    'empirical_se': plot_data['actual'].groupby(nb_bins).std() / nb_bins.value_counts().sort_index()
})

# Combine the statistics for the two models
rf_bin_stats['model'] = 'Random Forest'
nb_bin_stats['model'] = 'Naive Bayes'
bin_stats = pd.concat([rf_bin_stats, nb_bin_stats], ignore_index=True)

# Create the calibration plot
fig, ax = plt.subplots(figsize=(12, 8))

# Add perfect calibration line
ax.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration', linewidth=2)

# Plot Random Forest with ribbons
rf_data = bin_stats[bin_stats['model'] == 'Random Forest']
ax.scatter(rf_data['bin_center'], rf_data['empirical_prob'], 
          c='blue', alpha=0.7, label='Random Forest')
ax.fill_between(rf_data['bin_center'], 
                rf_data['empirical_prob'] - 1.96 * rf_data['empirical_se'],
                rf_data['empirical_prob'] + 1.96 * rf_data['empirical_se'],
                alpha=0.3, color='blue')

# Plot Naive Bayes with ribbons
nb_data = bin_stats[bin_stats['model'] == 'Naive Bayes']
ax.scatter(nb_data['bin_center'], nb_data['empirical_prob'], 
          c='green', alpha=0.7, label='Naive Bayes')
ax.fill_between(nb_data['bin_center'], 
                nb_data['empirical_prob'] - 1.96 * nb_data['empirical_se'],
                nb_data['empirical_prob'] + 1.96 * nb_data['empirical_se'],
                alpha=0.3, color='green')

# Formatting
ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_xlabel('Predicted Probability')
ax.set_ylabel('Empirical Probability')
ax.set_title(f'Calibration Plot with {n_bins} bins')
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

:::


In the figure above, the X-axis is the predicted probability and the Y-axis is the empirical probability. The points are the average predicted probability and the empirical probability in each bin. The ribbons are the 95% confidence intervals for the empirical probability.

How well-calibrated our models are can be assessed by comparing the points to the 45-degree red dashed line:

- **Perfect calibration**: Points lie on the 45-degree red dashed line
- **Overconfident**: Points lie below the line (predicted probabilities too high)  
- **Underconfident**: Points lie above the line (predicted probabilities too low)

A frequently asked question is how the bins are determined and the 10 points are chosen. First, the bins are chosen to be equal-width, where the maximum and minimum of the predicted probability are the boundaries. Then, the points are chosen to be the average predicted probability and the empirical probability in each bin. However, the specific method of choosing the bins and the points is not unique. For example, the `calibration_plot` function in R's `predtools` package buckets the predictions into equal "number" of observations per bin. This method results in different lengths between the points (see [this vignette](https://cran.r-project.org/web/packages/predtools/vignettes/calibPlot.html)) Creating calibration plots this way has a benefit of making the confidence intervals more informative, since it allows us to interpret the length of the confidence intervals not just as a scalar multiple of the standard "error", but as a scalar multiple of the standard "deviation" in the buckets. Therefore, the way these buckets are constructed should reflect the specific parameters the researcher is interested in, and it is not desirable to create calibration plots in just one fixed way.


### Adjusting the Bins

The confidence intervals in the figures above are calculated using the standard error of the empirical probability as mentioned above. Recall the standard error is calculated using the formula:

$$
\sigma = \frac{\text{standard deviation}}{\sqrt{n}}
$$

This results in wider confidence intervals for bins with fewer observations if the standard deviation stays the same. One should also note that the right number of bins is determined by the trade-off between the standard deviation and the number of observations in each bin. To see this, the following code chunk creates calibration plots with different numbers of bins: 2, 5, 10, and 20.


:::{.panel-tabset}

#### R

```{r adjusting-bins-r}
# Create calibration plots with different numbers of bins
n_bins <- c(2, 5, 10, 20)

for (n in n_bins) {
  # Bin the predictions
  rf_bins <- cut(plot_data$forest_pred, breaks = n, labels = FALSE, include.lowest = TRUE)
  nb_bins <- cut(plot_data$nb_pred, breaks = n, labels = FALSE, include.lowest = TRUE)

  # Calculate the statistics for the Random Forest model
  rf_bin_stats <- tibble(
    bin_center = tapply(plot_data$forest_pred, rf_bins, mean),
    empirical_prob = tapply(plot_data$actual, rf_bins, mean),
    count = as.numeric(table(rf_bins)),
    empirical_se = tapply(plot_data$actual, rf_bins, sd)/count
  )

  # Calculate the statistics for the Naive Bayes model
  nb_bin_stats <- tibble(
    bin_center = tapply(plot_data$nb_pred, nb_bins, mean),
    empirical_prob = tapply(plot_data$actual, nb_bins, mean),
    count = as.numeric(table(nb_bins)),
    empirical_se = tapply(plot_data$actual, nb_bins, sd)/count
  )

  # Combine the statistics for the two models
  bin_stats <-  mutate(rf_bin_stats, model = "Random Forest") |> 
    bind_rows(mutate(nb_bin_stats, model = "Naive Bayes"))

  # Create the calibration plot
  p <- ggplot(bin_stats) +
    geom_abline(aes(slope = 1, intercept = 0, color = "Perfect Calibration"), linetype = "dashed") +
    geom_ribbon(aes(x = bin_center, y = empirical_prob, fill = model,
                      ymin = empirical_prob - 1.96 * empirical_se, 
                      ymax = empirical_prob + 1.96 * empirical_se), alpha = 0.5) +
    geom_point(aes(x=bin_center, y=empirical_prob, color = model)) +
    scale_color_manual(values = c("Perfect Calibration" = "red", "Random Forest" = "blue", "Naive Bayes" = "green")) +
    scale_fill_manual(values = c("Perfect Calibration" = "red", "Random Forest" = "blue", "Naive Bayes" = "green")) +
    labs(title = paste0("Calibration Plot with ", n, " bins"), x=NULL, y=NULL, color = NULL) + guides(fill = "none") +
    theme_minimal() + theme(legend.position = "bottom")
  
  assign(paste0("calibration_plot_", n), p)
}

# Combine the calibration plots
calibration_plot <- calibration_plot_2 + calibration_plot_5 + calibration_plot_10 + calibration_plot_20

# Display the combined plot
calibration_plot
```

#### Python

```{python adjusting-bins-python}
#| fig-width: 12
#| fig-height: 8

# Create calibration plots with different numbers of bins
n_bins = [2, 5, 10, 20]

fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

for i, n in enumerate(n_bins):
    ax = axes[i]
    
    # Bin the predictions
    rf_bins = pd.cut(plot_data['forest_pred'], bins=n, labels=False, include_lowest=True)
    nb_bins = pd.cut(plot_data['nb_pred'], bins=n, labels=False, include_lowest=True)

    # Calculate the statistics for the Random Forest model
    rf_bin_stats = pd.DataFrame({
        'bin_center': plot_data['forest_pred'].groupby(rf_bins).mean(),
        'empirical_prob': plot_data['actual'].groupby(rf_bins).mean(),
        'count': rf_bins.value_counts().sort_index(),
        'empirical_se': plot_data['actual'].groupby(rf_bins).std() / rf_bins.value_counts().sort_index()
    })

    # Calculate the statistics for the Naive Bayes model
    nb_bin_stats = pd.DataFrame({
        'bin_center': plot_data['nb_pred'].groupby(nb_bins).mean(),
        'empirical_prob': plot_data['actual'].groupby(nb_bins).mean(),
        'count': nb_bins.value_counts().sort_index(),
        'empirical_se': plot_data['actual'].groupby(nb_bins).std() / nb_bins.value_counts().sort_index()
    })

    # Add perfect calibration line
    ax.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration', linewidth=2)

    # Plot Random Forest with ribbons
    ax.scatter(rf_bin_stats['bin_center'], rf_bin_stats['empirical_prob'], 
               c='blue', alpha=0.7, label='Random Forest')
    ax.fill_between(rf_bin_stats['bin_center'], 
                    rf_bin_stats['empirical_prob'] - 1.96 * rf_bin_stats['empirical_se'],
                    rf_bin_stats['empirical_prob'] + 1.96 * rf_bin_stats['empirical_se'],
                    alpha=0.3, color='blue')

    # Plot Naive Bayes with ribbons
    ax.scatter(nb_bin_stats['bin_center'], nb_bin_stats['empirical_prob'], 
              c='green', alpha=0.7, label='Naive Bayes')
    ax.fill_between(nb_bin_stats['bin_center'], 
                    nb_bin_stats['empirical_prob'] - 1.96 * nb_bin_stats['empirical_se'],
                    nb_bin_stats['empirical_prob'] + 1.96 * nb_bin_stats['empirical_se'],
                    alpha=0.3, color='green')

    # Formatting
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.set_xlabel('Predicted Probability')
    ax.set_ylabel('Empirical Probability')
    ax.set_title(f'Calibration Plot with {n} bins')
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

:::

Notice that as the number of bins increases, the confience intervals become wider; however, this is not always the case. If the bins are chosen to be decreasing the standard deviation in each bucket, the confidence intervals will become narrower. Making a good calibration plot requires careful consideration of the number of bins and the method of choosing the bins.

## Multiple Classes

For multinomial (multi-class) outcomes, we need to adapt the standard binary calibration approach. That is, we need to create a calibration plot for each class. We can do this in two approaches:

### 1. One-vs-Rest Calibration Plots
  1. Create separate calibration plots for each class using a one-vs-rest strategy:
  2. For each class, treat it as the "positive" class and all others as "negative"
  3. Plot predicted probabilities for that class against observed proportions
  4. Repeat for all classes, resulting in k plots for k classes


### 2. Class-wise Calibration Plots
  1. Bin the predicted probabilities for each class
  2. For each bin, calculate the observed frequency of that class
  3. Plot the predicted probability (x-axis) vs. observed frequency (y-axis) for each class
  4. Use different colors/markers for different classes on the same plot

In the following example, we will create the class-wise calibration plot. To do so, let us create a function and a toy dataset that plots a multinomial calibration plot.

:::{.panel-tabset}


#### R

```{r multinomial-calibration-plot-r}
# Create a toy dataset
x1 <- rnorm(1000)
x2 <- rnorm(1000)
  
# Create probabilities for 3 classes
# Class probabilities depend on the features
linear_pred1 <- 0.5 + 1.2 * x1 - 0.5 * x2
linear_pred2 <- -0.8 + 0.2 * x1 + 1.5 * x2
linear_pred3 <- 0.3 - 0.7 * x1 - 0.3 * x2
  
# Convert to probabilities using softmax
denom <- exp(linear_pred1) + exp(linear_pred2) + exp(linear_pred3)
prob1 <- exp(linear_pred1) / denom
prob2 <- exp(linear_pred2) / denom
prob3 <- exp(linear_pred3) / denom
  
# Generate class based on probabilities
classes <- apply(cbind(prob1, prob2, prob3), 1, function(p) sample(1:3, 1, prob = p))
class_factor <- factor(classes, levels = 1:3, labels = c("Class1", "Class2", "Class3"))
  
# Create dataset
data <- tibble(
  x1 = x1,
  x2 = x2,
  class = class_factor,
  true_prob1 = prob1,
  true_prob2 = prob2,
  true_prob3 = prob3
)
  

# Function for multinomial calibration plot
plot_multinomial_calibration <- function(predicted_probs, actual_class, n_bins=10) {

  predicted_probs <- as.data.frame(predicted_probs)
  
  classes <- colnames(predicted_probs)
  plot_data <- data.frame()
  
  for (class_name in classes) {
    # Extract probabilities for this class
    probs <- as.numeric(predicted_probs[[class_name]])
    
    # Create bins
    bins <- cut(probs, breaks=seq(0, 1, length.out=n_bins+1), include.lowest=TRUE)
    
    # Calculate observed proportions and standard errors
    bin_summary <- aggregate(as.numeric(actual_class == class_name), by=list(bin=bins), FUN=mean)
    bin_centers <- aggregate(probs, by=list(bin=bins), FUN=mean)
    bin_counts <- aggregate(as.numeric(actual_class == class_name), by=list(bin=bins), FUN=length)
    bin_sd <- aggregate(as.numeric(actual_class == class_name), by=list(bin=bins), FUN=sd)
    
    # Calculate standard error
    bin_se <- bin_sd$x / sqrt(bin_counts$x)
    
    # Combine data
    class_data <- data.frame(
      class = class_name,
      bin_center = bin_centers$x,
      observed_prop = bin_summary$x,
      count = bin_counts$x,
      empirical_se = bin_se
    )
    plot_data <- rbind(plot_data, class_data)
  }
  
  # Create plot with confidence intervals
  ggplot(plot_data, aes(x=bin_center, y=observed_prop, color=class)) +
    geom_abline(slope=1, intercept=0, linetype="dashed", color="red") +
    geom_ribbon(aes(ymin = observed_prop - 1.96 * empirical_se, 
                    ymax = observed_prop + 1.96 * empirical_se, 
                    fill = class), alpha = 0.3) +
    geom_point(aes(size = count)) +
    geom_line() +
    xlim(0, 1) + ylim(0, 1) +
    labs(x="Predicted Probability", y="Observed Proportion", 
         title="Calibration Plot for Multinomial Classes",
         color = "Class", fill = "Class", size = "Sample Size") +
    guides(fill = "none", size = "none") +
    theme_minimal() +
    theme(legend.position = "bottom")
}
```

#### Python

```{python multinomial-calibration-plot-python} 
# Create a toy dataset
x1 = np.random.normal(size=1000)
x2 = np.random.normal(size=1000)

# Create probabilities for 3 classes
# Class probabilities depend on the features
linear_pred1 = 0.5 + 1.2 * x1 - 0.5 * x2
linear_pred2 = -0.8 + 0.2 * x1 + 1.5 * x2
linear_pred3 = 0.3 - 0.7 * x1 - 0.3 * x2

# Convert to probabilities using softmax
denom = np.exp(linear_pred1) + np.exp(linear_pred2) + np.exp(linear_pred3)
prob1 = np.exp(linear_pred1) / denom
prob2 = np.exp(linear_pred2) / denom
prob3 = np.exp(linear_pred3) / denom

# Generate class based on probabilities
# Stack probabilities for all classes
prob_matrix = np.column_stack([prob1, prob2, prob3])
classes = np.array([np.random.choice(3, p=prob_matrix[i]) for i in range(1000)])
class_factor = np.array(classes, dtype=object)
class_factor[classes == 0] = "Class1"
class_factor[classes == 1] = "Class2"
class_factor[classes == 2] = "Class3"

# Create dataset
data = pd.DataFrame({
    'x1': x1,
    'x2': x2,
    'class': class_factor,
    'true_prob1': prob1,
    'true_prob2': prob2,
    'true_prob3': prob3
})


# Function for multinomial calibration plot with confidence intervals
def plot_multinomial_calibration(y_true, y_pred_proba, class_names=None, n_bins=10):
    """
    y_true: array-like of true class labels (integers)
    y_pred_proba: array-like of shape (n_samples, n_classes) with predicted probabilities
    class_names: list of class names (optional)
    n_bins: number of bins for probability
    """
    n_classes = y_pred_proba.shape[1]
    
    if class_names is None:
        class_names = [f"Class {i}" for i in range(n_classes)]
    
    plt.figure(figsize=(12, 8))
    plt.plot([0, 1], [0, 1], 'r--', label='Perfectly calibrated', linewidth=2)
    
    colors = ['blue', 'green', 'orange', 'purple', 'brown']
    
    for i in range(n_classes):
        # One-vs-rest approach: is this sample class i or not?
        # Convert class names to integers for comparison
        y_true_numeric = np.array([0 if label == "Class1" else 1 if label == "Class2" else 2 for label in y_true])
        y_true_binary = (y_true_numeric == i).astype(int)
        y_prob = y_pred_proba.iloc[:, i]
        
        # Create bins manually to calculate confidence intervals
        bins = pd.cut(y_prob, bins=n_bins, labels=False, include_lowest=True)
        
        # Calculate bin statistics
        bin_stats = []
        for bin_idx in range(n_bins):
            mask = bins == bin_idx
            if mask.sum() > 0:
                bin_center = y_prob[mask].mean()
                observed_prop = y_true_binary[mask].mean()
                count = mask.sum()
                std_error = np.sqrt(observed_prop * (1 - observed_prop) / count) if count > 0 else 0
                
                bin_stats.append({
                    'bin_center': bin_center,
                    'observed_prop': observed_prop,
                    'count': count,
                    'std_error': std_error
                })
        
        bin_stats = pd.DataFrame(bin_stats)
        
        if len(bin_stats) > 0:
            color = colors[i % len(colors)]
            
            # Plot confidence intervals
            plt.fill_between(bin_stats['bin_center'], 
                            bin_stats['observed_prop'] - 1.96 * bin_stats['std_error'],
                            bin_stats['observed_prop'] + 1.96 * bin_stats['std_error'],
                            alpha=0.2, color=color)
            
            # Plot points and line
            plt.scatter(bin_stats['bin_center'], bin_stats['observed_prop'], 
                       s=bin_stats['count']*2, color=color, alpha=0.7, label=class_names[i])
            plt.plot(bin_stats['bin_center'], bin_stats['observed_prop'], 
                    color=color, linewidth=2)
    
    plt.xlabel('Predicted Probability')
    plt.ylabel('Observed Proportion')
    plt.title('Calibration Plot for Multinomial Classes')
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.legend(loc='best')
    plt.grid(True, alpha=0.3)
    plt.show()
```

:::



We then train a multinomial logistic regression model on the first 700 observations with covariates $x_1$ and $x_2$.


:::{.panel-tabset}

#### R

```{r train-model-r}
# Split the data into training and testing sets
train_data <- data[1:700, ]
test_data <- data[701:1000, ]

# Train a multinomial logistic regression model
model <- multinom(class ~ x1 + x2, data = train_data)

pred_probs <- predict(model, newdata = test_data, type = "probs")

# Convert to a tibble
pred_probs_df <- as_tibble(pred_probs)
colnames(pred_probs_df) <- levels(test_data$class)
```


#### Python

```{python train-model-python}
# Split the data into training and testing sets
train_data = data.iloc[:700]
test_data = data.iloc[701:1000]

# Train a multinomial logistic regression model
model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
model.fit(train_data[['x1', 'x2']], train_data['class'])

pred_probs = model.predict_proba(test_data[['x1', 'x2']])

# Convert to a tibble
pred_probs_df = pd.DataFrame(pred_probs, columns=model.classes_)

```

:::



:::{.panel-tabset}

#### R

```{r plotting-r}
#| fig-width: 12
#| fig-height: 8
#| 
# Create the calibration plot
plot_multinomial_calibration(pred_probs_df, test_data$class, n_bins=10)
```

#### Python

```{python plotting-python}
#| fig-width: 12
#| fig-height: 8

# Create the calibration plot
plot_multinomial_calibration(test_data['class'], pred_probs_df, n_bins=10)
```

:::